Exercice 1 :

    Question 1.1 :

        1. Map Input Records : correspond au nombre de couples <cle,valeur> en entree de map.
           Map Output Records : correspond au nombre de couples <cle,valeur> en sortie de map.

        2. reduce input records correspond:
			- soit au nombre de couples <cle,valeur> produit par map en l'absence de combiner.
                        - soit au nombre de couples <cle,valeur> produit par combine.
           Donc la valeur de "Map output records" est la même que la valeur de "Reduce input records" dans l absence de combiner.
 
        3. Apres un map, un traitement consistent à regrouper toutes les valeurs d'une même clé est effectué.
	   Le reduce qui suit le map utilise les couples <cle,liste(valeurs)> produit lors du traitement intermediaire.
           reduce input groups correspond au nombre de groupes ainsi calculé (le nombre de clé sans doublon).

    Question 1.2 :

          Le chemin vers le reperetoire personnel dans HDFS = /user/<nom_user> (avec hdfs dfs -ls /user)
          Dans notre cas /user/marouf .

     Question 1.3 :

          Le nombre de "splits" lus sur HDFS est 5 splits ou blocs, 1 pour chacun des 5 tomes des miserables.
          Le compteur "number of splits" indique cette valeur.

     Question 1.4 :

         1. Chaque reducer génére indépendemment un fichier résultat. Une fois que les mappers finissent de s'éxécuter, il n y a plus de raison de synchroniser les reducers qui étaient en attente du 
        regroupement des clés. Chaque reducer peut gérer une ou un groupe de clé indépendemment des autres.
	Comme on a fixé le nombre de reducers à 3, on retrouve 3 fichiers résultats dans le répértoire.
		
 	    La politique de partitionnement Hadoop explique pourquoi il n y a qu'un seul reducer dans le cas ou ne spécifie pas explicitement le nombre de reducers à éxécuter.
        Par défaut, Hadoop en ajoute autant qu'il le juge utile.

         2. les compteurs qui permettent de vérifier que le combiner a fonctionné sont "Combine input records" et "Combine output records", qui indiquent le nombre de couples <clé, valeur> en entrée et sortie du 
        combiner et qui doivent être différent à zero.

         3. Dans le cas des 5 tomes des misérables, l'utilisation du combiner divise le nombre de données à traiter pour les reducers d'un facteur à peu prés égal à 4.
	On retrouve ce facteur sur la quantité de données écrite sur le disk en sortie de map (compteur "Map output materialized bytes"):
				* 4977046 bytes sans combiner
                                * 1303867 bytes avec combiner 

        4. avec la commande unix: sort -r -n -k 2 <fichier_resultat>
       Le premier couple <clé,valeur> indiqué en resultat et le mot le plus utilisé, dans notre cas, le mot le plus utilisé est: "de" avec un total d'occurences de 16757.

Exercice 2 :

     Question 2.2 :

         1. le type de données intermédiaire pour pouvoir utiliser un combiner est:
             - 1 champ pour le tag de type TEXT
             - 1 champ pour le nombre d'occurence de type Integer

        Le map retournerait donc est un couple < clé , valeur > avec:
             - la clé étant l'identifiant du pays
             - la valeur étant la structure de données i.e les tags et leurs nombre d'occurences

         2. Les 5 tags les plus utilisés en France sont:
	     - france 
             - spain 
	     - europe 
	     - españa 
             - bretagne 

         3. Ne connaissant pas a priori la taille de la structure (HashMap), un processus peut essayer d'utiliser plus de resources que disponible. Une exception OutOfMemory devrait être levé lors de l'éxécution.

